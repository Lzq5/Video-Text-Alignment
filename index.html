<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

  <style type="text/css">
    @font-face {
      font-family: 'Avenir Book';
      src: url("./fonts/Avenir_Book.ttf");
      /* File to be stored at your site */
    }

    body {
      font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight: 300;
      font-size: 14px;
      margin-left: auto;
      margin-right: auto;
      width: 800px;
    }

    h1 {
      font-weight: 300;
    }

    h2 {
      font-weight: 300;
    }

    p {
      font-weight: 300;
      line-height: 1.4;
    }

    code {
      font-size: 0.8rem;
      margin: 0 0.2rem;
      padding: 0.5rem 0.8rem;
      white-space: nowrap;
      background: #efefef;
      border: 1px solid #d3d3d3;
      color: #000000;
      border-radius: 3px;
    }

    pre>code {
      display: block;
      white-space: pre;
      line-height: 1.5;
      padding: 0;
      margin: 0;
    }

    pre.prettyprint>code {
      border: none;
    }


    .container {
      display: flex;
      align-items: center;
      justify-content: center
    }

    .image {
      flex-basis: 40%
    }

    .text {
      padding-left: 20px;
      padding-right: 20px;
    }

    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
      padding: 20px;
    }

    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.rounded {
      border: 0px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;

    }

    a:link,
    a:visited {
      color: #1367a7;
      text-decoration: none;
    }

    a:hover {
      color: #208799;
    }

    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }

    .layered-paper-big {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
      /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 45px;
    }


    .layered-paper {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
      /* The third layer shadow */
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
    }

    .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
    }

    hr {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
  </style>
  <title>A Strong Baseline for Temporal Video-Text Alignment
  </title>
</head>

<body>
  <br>
  <center>
    <span style="font-size:32px">A Strong Baseline for Temporal Video-Text Alignment
    </span><br><br><br>
  </center>
  <table align="center">
    <tbody>
      <tr>
        <td align="center" width="200px">
          <center>
            <span style="font-size:16px">Zeqian Li<sup>1*</sup></span>
          </center>
        </td>
        <td align="center" width="200px">
          <center>
            <span style="font-size:16px">Qirui Chen<sup>1*</sup></span>
          </center>
        </td>
        <td align="center" width="200px">
          <center>
            <span style="font-size:16px"><a href="https://tengdahan.github.io/">Tengda Han</a><sup>2</sup></span>
          </center>
        </td>
      </tr>
    </tbody>
  </table><br>
  <table align="center">
    <tbody>
      <tr>
        <td align="center" width="200px">
          <center>
            <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1</sup></span>
          </center>
        </td>
        <td align="center" width="200px">
          <center>
            <span style="font-size:16px"><a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang</a><sup>1</sup></span>
          </center>
        </td>
        <td align="center" width="200px">
          <center>
            <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2</sup></span>
          </center>
        </td>
      </tr>
    </tbody>
  </table><br>

  <table align="center" width="700px">
    <tbody>
      <tr>
        <td align="center" width="400px">
          <span style="font-size:16px"><sup>1</sup>Coop. Medianet Innovation Center, Shanghai Jiao Tong
            University</span>
        </td>
      </tr>
    </tbody>
  </table>
  <table align="center" width="700px">
    <tbody>
      <tr>
        <td align="center" width="400px">
          <center>
            <span style="font-size:16px"><sup>2</sup>Visual Geometry Group, University of Oxford</span>
          </center>
        </td>
      </tr>
    </tbody>
  </table>

  <table align="center" width="600px">
    <tbody>
      <tr>
        <td align="center" width="200px">
          <center>
            <br>
            <span style="font-size:20px">Code
              <a href="https://github.com/Lzq5/Video-Text-Alignment"> [GitHub]</a>
            </span>
          </center>
        </td>

        <td align="center" width="200px">
          <center>
            <br>
            <span style="font-size:20px">Paper 
              <a href="https://arxiv.org/abs/2312.14055"> [arXiv]</a>
            </span>
          </center>
        </td>

        <td align="center" width="200px">
          <center>
            <br>
            <span style="font-size:20px">Cite 
              <a href="./resources/bibtex.txt"> [BibTeX]</a>
            </span>
          </center>
        </td>

      </tr>
    </tbody>
  </table>

  <br>
  <hr>
  <center>
    <h2> Abstract </h2>
  </center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <left>
      In this paper, we consider the problem of temporally aligning the video and texts from instructional videos, specifically, given a long-term video, and associated text sentences, our goal is to determine their corresponding timestamps in the video.
      To this end, we establish a simple, yet strong model that adopts a Transformer-based architecture with all texts as queries, iteratively attending to the visual features, to infer the optimal timestamp. We conduct thorough experiments to investigate: 
      (i) the effect of upgrading ASR systems to reduce errors from speech recognition, 
      (ii) the effect of various visual-textual backbones, ranging from CLIP to S3D, to the more recent InternVideo, 
      (iii) the effect of transforming noisy ASR transcripts into descriptive steps by prompting a large language model (LLM), to summarize the core activities within the ASR transcript as a new training dataset. 
      As a result, our proposed simple model demonstrates superior performance on both narration alignment and procedural step grounding tasks, surpassing existing state-of-the-art methods by a significant margin on three public benchmarks, namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask. 
      We believe the proposed model and dataset with descriptive steps can be treated as a strong baseline for future research in temporal video-text alignment. All codes, models, and the resulting dataset will be publicly released to the research community.
    </left>
  </p>

  <br>
  <hr>
  <center>
    <h2> Architecture </h2>
  </center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <p><div style="text-align: center;"><img class="left" src="./resources/model.jpg" width="400px"></div></p>
    <p>
      <left>
        Schematic visualization of the proposed Transformer-based video-text alignment network termed <b>NSVA</b>.
        The visual features are treated as key-value pairs while textual features are treated as queries, to predict the alignment score matrix between video frames and texts. </left>
    </p>


    <br>
    <hr>
    <center>
      <h2> ASR Transcripts &nbsp;&rarr;&nbsp; Aligned Descriptive Steps </h2>
    </center>
    <p style="text-align:justify; text-justify:inter-ideograph;">
      <p>
        <left>
          As presented in the following figure, 
          the entire procedure of ASR transcripts transformation can be divided into three parts: 
          (i) we leverage LLM to summarize narrations from the ASR transcript into descriptive steps; (ii) we use the similarity between the original transcript and generated steps to roughly determine the start/end timestamp for each step as pseudo-label; 
          (iii) we train the <b>NSVA</b> model on the generated steps with pseudo-label, and then use the trained model to refine the time range for each generated step (i.e., self-training). 
          We name the final dataset as <b>HowToStep</b>.
        </left>
      </p>
      <p><img class="left" src="./resources/pipeline.jpg" width="800px"></p>
      <br>
      <hr>
      <center>
        <h2>Results</h2>
      </center>
      <p>
        <left>
          We compare our best model with existing state-of-the-art approaches, to present a strong, yet simple baseline on video-text alignment for future research. 
          As shown in the table, on the challenging HT-Step task, 
          that aims to ground unordered procedural steps in videos, 
          our model achieves 46.7% R@1, leading to an absolute improvement of 9.3%, over the existing state-of-the-art (37.4%) achieved by VINA;
          On HTM-Align, which aligns narrations in the video, 
          our method exceeds sota model by 3.4%; 
          On CrossTask, where we need to align video frames and task-specific steps without finetuning, our method outperforms existing state-of-the-art approach by 4.7%,
          demonstrating our model learns stronger joint video-text representation.</left>
      </p>
      <center>
        <p><img class="center" src="./resources/results.jpg" width="400px"></p>
      </center>
      <br>
      <hr>
      <center>
        <h2> Acknowledgements </h2>
      </center>
      <p>
        Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a
          href="http://richzhang.github.io/">Richard Zhang</a>.
      </p>
      <br>
</body>

</html>